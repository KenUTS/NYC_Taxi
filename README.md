# NYC_Taxi

The primary aim of this project is to assist taxi drivers in New York City in optimizing their earnings by utilizing statistical characteristics such as the month of the year, day of the week, hour of the day, duration, distance, and pickup places. Moreover, the proportion of tips earned per trip will be a significant component of the driver's earnings. In order to optimize their earnings, taxi drivers will be provided with strategies aimed at maximizing their income by considering the time of each trip. Finally, the utilization of machine learning methods, specifically Decision Tree and Random Forest, is employed to forecast the forthcoming monetary remuneration that drivers will obtain each individual trip.

## Steps
1. Data loading: The initial stage of this project involves establishing a Databricks workspace to facilitate data storage and warehousing. The provided Google Drive link facilitates the integration of datasets spanning from 2015 to 2022 into the Databricks platform for storage purposes. Once all the provided datasets have been loaded, they will be merged into two distinct datasets, namely the green dataset and the yellow dataset.
2. Data cleaning: Once all the provided datasets have been loaded, they are merged into two distinct datasets, namely the green dataset and the yellow dataset. The cleaning processes start at removing unrealistic trips, there are 8 filters have created to remove the following trips:
- Trips finishing before the starting time, by looking at drop off time is earlier than pick up time.
- Trips where the pickup/dropoff datetime is outside of the range, by looking at pickup/dropoff datetime is outside of the file year which is contained in file name.
- Trips with negative speed by looking at speed is smaller than 0.
- Trips with very high speed (look for NYC and outside of NYC speed limit ), by looking at speed is faster than 85 km/h based on NYC transport rules.
- Trips that are traveling too short or too long (duration wise) by looking at time travel are smaller than 1 minute and greater than 3 hours based on Google Maps in selected areas.
- Trips that are traveling too short or too long (distance wise) by looking at travel distance is smaller than 0.5 km and greater than 100 km based on Google Map in selected areas.
- Trips with very low speed by looking at speed is lower than 5 km/h.
- Remove duplicate records in both datasets.
3. Data storage formats: Once two datasets have been cleaned, they are merged into two distinct datasets, namely the green dataset and the yellow dataset. The "union" function is employed to merge the datasets into a single dataset, which is subsequently merged with the location file using a left join operation. There exists a substantial disparity in terms of size when comparing the act of saving data in CSV format against Parquet format. As an illustration, the green taxi dataset in the year 2015 was stored in a compressed format, occupying a total of 404,556,105 bytes. In contrast, the file sizes of the CSV files were calculated to be the sum of 718,297,967 bytes, 716,001,938 bytes, and 744,817,515 bytes, respectively. Consequently, the ultimate dataset is stored in the parquet format.
4. Data exploration: Once the completed dataset has been stored, it is subsequently employed for business-related objectives. In this stage, the SQL Spark framework is employed to analyze the dataset in order to address business inquiries, as elucidated in the subsequent section.
5. Data models: Following the completion of data exploration research, the subsequent crucial phase is constructing two machine learning models with the objective of predicting the aggregate monetary compensation received by drivers for individual trips over the months of October, November, and December in the year 2022. In this step, various data preparations, such as cleaning and converting, are performed:
- Data cleaning: The dataset includes columns that contain a significant number of non-null values, whereas columns that do not contribute to the prediction of the target variable have been excluded. The dataset includes many attributes such as "VendorID", "DOLocationID", "PULocationID", "pickup_datetime", "dropoff_datetime", "store_and_fwd_flag", "passenger_count", "fare_amount" ,"payment_type", "congestion_surcharge", "ehail_fee","trip_type", "airport_fee", "PUZone", "PUservice_zone", "DOZone", "DOservice_zone".
- Pipeline: A stage has been implemented to execute various transformation processes, including One-hot encoding and StringIndexer. These processes are iterated to convert categorical variables, such as color and RateCodeID, into integer variables. The RateCodeID ranges from 1 to 6; nevertheless, it does not indicate a higher final rate. Consequently, the data will be transformed back into its original categorical form and afterwards utilized for the process of one-hot encoding. A feature column is thereafter generated by selecting all relevant features for this particular project.
- Data storage: The testing, training and validating data sets are saved in parquet file format for modeling.
- Baseline: The dataset obtained from part 2, is combined with the main dataset in order to identify the target base of the dataset.
- Remove Outliers: There are significant outliers in total amount columns, so they will be removed for training and validating dataset.
- Data sampling: In order to accommodate the size of the dataset, a subset of training and validation data is extracted for the purpose of model fitting.
6. Model performance: The RMSE metric score is utilized to evaluate the performance of different models. The model that exhibits the highest performance is then selected for comparison with the baseline model.
